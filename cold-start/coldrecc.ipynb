{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trunkooze/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load data\n",
    "clean_folder = \"clean\"\n",
    "books_df = pd.read_csv(os.path.join(clean_folder, \"books.csv\"))\n",
    "book_tags_df = pd.read_csv(os.path.join(clean_folder, \"book_tags.csv\"))\n",
    "tags_df = pd.read_csv(os.path.join(clean_folder, \"tags.csv\"))\n",
    "\n",
    "# # Standardize column name for merging\n",
    "# book_tags_df.rename(columns={'book_id': 'book_id'}, inplace=True)\n",
    "\n",
    "# # Rename 'book_id' in books_df to 'book_id' for merging\n",
    "# books_df.rename(columns={'book_id': 'book_id'}, inplace=True)\n",
    "\n",
    "# Merge book_tags with tag names\n",
    "book_tags_merged = pd.merge(book_tags_df, tags_df, on='tag_id', how='inner')\n",
    "\n",
    "# Merge with books\n",
    "books_with_tags = pd.merge(\n",
    "    books_df[['book_id', 'title', 'authors', 'average_rating',\n",
    "              'ratings_count', 'original_publication_year', 'language_code']],\n",
    "    book_tags_merged,\n",
    "    on='book_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Group by book and concatenate tag names into a single string\n",
    "books_tagged = books_with_tags.groupby('book_id').agg({\n",
    "    'title': 'first',\n",
    "    'authors': 'first',\n",
    "    'original_publication_year': 'first',\n",
    "    'language_code': 'first',\n",
    "    'tag_name': lambda x: ' '.join(set(x))  # deduplicated tag list\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = SentenceTransformer('BAAI/bge-m3')\n",
    "\n",
    "# Encode tag text for each book\n",
    "books_tagged['tag_text'] = books_tagged['tag_name']\n",
    "book_tag_embeddings0 = model.encode(books_tagged['tag_text'].tolist(), convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_multiple_genres(user_genres, top_n=10):\n",
    "    \"\"\"\n",
    "    user_genres: str — comma-separated genres, e.g., \"Fantasy, Mystery, Romance\"\n",
    "    top_n: int — number of results to return\n",
    "    \"\"\"\n",
    "    # Parse and clean genres\n",
    "    genre_list = [g.strip() for g in user_genres.split(',') if g.strip()]\n",
    "    \n",
    "    if not genre_list:\n",
    "        raise ValueError(\"Please input at least one genre!\")\n",
    "\n",
    "    # Embed each genre separately\n",
    "    genre_embeddings = model.encode(genre_list, convert_to_tensor=True)\n",
    "\n",
    "    # Compute average embedding (user profile)\n",
    "    user_embedding = genre_embeddings.mean(dim=0)\n",
    "\n",
    "    # Compute cosine similarity with all books\n",
    "    scores = util.pytorch_cos_sim(user_embedding, book_tag_embeddings0)[0]\n",
    "    top_results = scores.topk(top_n)\n",
    "\n",
    "    # Extract matching rows\n",
    "    results = books_tagged.iloc[top_results[1].cpu().numpy()].copy()\n",
    "    results['similarity'] = top_results[0].cpu().numpy()\n",
    "    return results[[\n",
    "        'book_id', 'title', 'authors', 'original_publication_year',\n",
    "        'language_code', 'similarity'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      book_id                                         title  \\\n",
      "6637     6638              Cross My Heart (Alex Cross, #21)   \n",
      "5691     5692             Alex Cross, Run (Alex Cross, #20)   \n",
      "4021     4022               Cross Country (Alex Cross, #14)   \n",
      "3692     3693            Buried Prey (Lucas Davenport, #21)   \n",
      "4489     4490              Kill Alex Cross (Alex Cross #18)   \n",
      "6293     6294                    Private Games (Private #3)   \n",
      "3516     3517  The 8th Confession (Women's Murder Club, #8)   \n",
      "5890     5891                 Hope to Die (Alex Cross, #22)   \n",
      "2985     2986                  Hidden (House of Night, #10)   \n",
      "7937     7938      I, Michael Bennett (Michael Bennett, #5)   \n",
      "\n",
      "                                authors  original_publication_year  \\\n",
      "6637                    James Patterson                     2013.0   \n",
      "5691                    James Patterson                     2013.0   \n",
      "4021                    James Patterson                     2008.0   \n",
      "3692                      John Sandford                     2011.0   \n",
      "4489                    James Patterson                     2011.0   \n",
      "6293  James Patterson, Mark T. Sullivan                     2012.0   \n",
      "3516     James Patterson, Maxine Paetro                     2009.0   \n",
      "5890                    James Patterson                     2014.0   \n",
      "2985            P.C. Cast, Kristin Cast                     2012.0   \n",
      "7937  James Patterson, Michael Ledwidge                     2012.0   \n",
      "\n",
      "     language_code  similarity  \n",
      "6637           eng    0.645569  \n",
      "5691          None    0.625470  \n",
      "4021           eng    0.612076  \n",
      "3692           eng    0.602501  \n",
      "4489           eng    0.601077  \n",
      "6293           eng    0.598970  \n",
      "3516          None    0.598963  \n",
      "5890           eng    0.591352  \n",
      "2985         en-US    0.590398  \n",
      "7937           eng    0.590019  \n"
     ]
    }
   ],
   "source": [
    "user_input = \"Fantasy, Paranormal, Fiction, Science Fiction, Graphic Novels, Novel, Urban Fiction\"\n",
    "recommendations = recommend_by_multiple_genres(user_input, top_n=10)\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv(os.path.join(clean_folder, \"ratings.csv\"))\n",
    "to_read_test_df = pd.read_csv(os.path.join(clean_folder, \"to_read_test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eligible users: 16150\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# 4. Filter Users + Sample\n",
    "# ------------------------------------\n",
    "#   We'll keep only users who rated >60 and <160 books,\n",
    "#   then randomly sample 20% of those rating rows.\n",
    "\n",
    "# Count how many ratings each user did\n",
    "user_counts = ratings_df.groupby('user_id')['book_id'].count().reset_index()\n",
    "user_counts.rename(columns={'book_id': 'count_ratings'}, inplace=True)\n",
    "\n",
    "# Keep those who rated between 60 and 160\n",
    "eligible_users = user_counts[\n",
    "    (user_counts['count_ratings'] > 30) &\n",
    "    (user_counts['count_ratings'] < 100)\n",
    "]\n",
    "print(f\"Eligible users: {len(eligible_users)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to keep only those users' ratings\n",
    "filtered_ratings = pd.merge(\n",
    "    ratings_df,\n",
    "    eligible_users[['user_id']],\n",
    "    on='user_id',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered user count: 16150\n",
      "Filtered ratings shape: (1355729, 3)\n",
      "User IDs in filtered ratings:\n",
      "[    2     6     8 ... 52013 33111 49802]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Filtered user count: {len(eligible_users)}\")\n",
    "print(f\"Filtered ratings shape: {filtered_ratings.shape}\")\n",
    "\n",
    "# print user id in filtered ratings\n",
    "print(\"User IDs in filtered ratings:\")\n",
    "print(filtered_ratings['user_id'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "# ------------------------------------\n",
    "# input test file\n",
    "# loop to all user\n",
    "# get user rated book from ratings.csv\n",
    "# get tag from first 3 book compare to tag_id in UI_tag.csv most match use that genre as a genre to test as user_test_genre\n",
    "# get first 5 book from to_read_test.csv as wishlist (will be goal of evaluation)\n",
    "# get recommendation from recommend_by_multiple_genres(user_test_genre, 5)\n",
    "# compare the recommendation with wishlist score using dcg\n",
    "# sum up all the score\n",
    "# end loop\n",
    "# average the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  11%|█         | 1774/16150 [00:09<01:35, 149.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 2129 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  12%|█▏        | 2015/16150 [00:10<01:05, 214.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 3336 is in the wishlist!\n",
      "Recommended Book ID 3687 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  13%|█▎        | 2087/16150 [00:11<01:06, 212.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 4490 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  16%|█▌        | 2529/16150 [00:13<01:15, 181.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 6294 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  17%|█▋        | 2673/16150 [00:14<01:05, 205.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 1098 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  18%|█▊        | 2833/16150 [00:15<01:14, 178.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 286 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  19%|█▊        | 3028/16150 [00:16<01:24, 155.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 3589 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  19%|█▉        | 3080/16150 [00:17<01:22, 157.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 3336 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  23%|██▎       | 3746/16150 [00:20<01:20, 154.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 3878 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  24%|██▍       | 3889/16150 [00:21<01:06, 184.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 5970 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  24%|██▍       | 3908/16150 [00:21<01:09, 177.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 389 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  27%|██▋       | 4360/16150 [00:24<01:14, 157.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 8381 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  28%|██▊       | 4572/16150 [00:26<01:36, 119.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 389 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  30%|███       | 4859/16150 [00:27<01:04, 175.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 7728 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  30%|███       | 4900/16150 [00:28<01:13, 154.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 2875 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  31%|███       | 5025/16150 [00:28<01:05, 169.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 9596 is in the wishlist!\n",
      "Recommended Book ID 286 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  32%|███▏      | 5235/16150 [00:29<00:59, 183.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 225 is in the wishlist!\n",
      "Recommended Book ID 130 is in the wishlist!\n",
      "Recommended Book ID 3064 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  38%|███▊      | 6098/16150 [00:35<01:14, 134.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 3064 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  41%|████      | 6573/16150 [00:37<01:00, 158.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 286 is in the wishlist!\n",
      "Recommended Book ID 286 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  42%|████▏     | 6789/16150 [00:39<00:59, 157.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 286 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  44%|████▎     | 7029/16150 [00:40<00:59, 152.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 2684 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  44%|████▍     | 7096/16150 [00:41<00:55, 162.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 6294 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  46%|████▌     | 7451/16150 [00:43<00:59, 145.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 389 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  52%|█████▏    | 8469/16150 [00:52<00:47, 160.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 3064 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  53%|█████▎    | 8502/16150 [00:52<01:12, 105.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 1098 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  55%|█████▌    | 8922/16150 [00:56<00:54, 133.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 5178 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  56%|█████▋    | 9112/16150 [00:58<01:31, 76.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 8971 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  57%|█████▋    | 9283/16150 [00:59<01:02, 109.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 8797 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  61%|██████    | 9865/16150 [01:04<01:11, 88.37it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 2986 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  62%|██████▏   | 9968/16150 [01:05<00:35, 172.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 8911 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  63%|██████▎   | 10245/16150 [01:08<01:01, 96.68it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 1919 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  65%|██████▌   | 10502/16150 [01:10<01:07, 84.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 6450 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  66%|██████▌   | 10692/16150 [01:12<00:50, 108.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 5352 is in the wishlist!\n",
      "Recommended Book ID 3044 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  72%|███████▏  | 11571/16150 [01:20<00:46, 97.46it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 389 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  74%|███████▍  | 11974/16150 [01:23<00:34, 121.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 3589 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  76%|███████▌  | 12298/16150 [01:26<00:42, 89.69it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 1080 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  78%|███████▊  | 12631/16150 [01:29<00:29, 118.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 389 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  79%|███████▊  | 12699/16150 [01:30<00:30, 113.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 389 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  82%|████████▏ | 13224/16150 [01:35<00:37, 78.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 9876 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  88%|████████▊ | 14145/16150 [01:43<00:19, 104.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 92 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  88%|████████▊ | 14212/16150 [01:43<00:18, 104.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 389 is in the wishlist!\n",
      "Recommended Book ID 286 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  91%|█████████ | 14716/16150 [01:49<00:12, 117.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 8971 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  94%|█████████▍| 15211/16150 [01:53<00:06, 150.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 286 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  94%|█████████▍| 15251/16150 [01:53<00:06, 138.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 286 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  95%|█████████▌| 15398/16150 [01:55<00:07, 99.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 5960 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  97%|█████████▋| 15735/16150 [01:58<00:04, 92.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 8268 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  99%|█████████▊| 15938/16150 [02:00<00:01, 115.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 7580 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG:  99%|█████████▉| 15960/16150 [02:01<00:02, 85.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book ID 1590 is in the wishlist!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG: 100%|██████████| 16150/16150 [02:02<00:00, 131.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average DCG Score: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Evaluation BEST ONE\n",
    "# ------------------------------------\n",
    "# input test file\n",
    "# loop to all user\n",
    "# get user rated book from ratings.csv\n",
    "# get tag from first 3 book compare to tag_id in UI_tag.csv most match use that genre as a genre to test as user_test_genre\n",
    "# get first 5 book from to_read_test.csv as wishlist (will be goal of evaluation)\n",
    "# get recommendation from recommend_by_multiple_genres(user_test_genre, 5)\n",
    "# compare the recommendation with wishlist score using dcg\n",
    "# sum up all the score\n",
    "# end loop\n",
    "# average the score\n",
    "# ------------------------------------\n",
    "\n",
    "def evaluate_dcg(eligible_users, ratings_df, books_tagged, to_read_test, top_n=5):\n",
    "    \"\"\"\n",
    "    Evaluate the DCG score for recommendations.\n",
    "\n",
    "    Parameters:\n",
    "    - eligible_users: DataFrame containing eligible user IDs.\n",
    "    - ratings_df: DataFrame containing user ratings.\n",
    "    - books_tagged: DataFrame containing books with their tags.\n",
    "    - to_read_test: DataFrame containing users' wishlist books.\n",
    "    - top_n: Number of recommendations to consider for DCG calculation.\n",
    "\n",
    "    Returns:\n",
    "    - average_dcg_score: The average DCG score across all users.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables for evaluation\n",
    "    total_dcg_score = 0\n",
    "    user_count = 0\n",
    "\n",
    "    # Loop through all eligible users with tqdm progress bar\n",
    "    for user_id in tqdm(eligible_users['user_id'], desc=\"Evaluating DCG\"):\n",
    "        # Get books rated by the user\n",
    "        user_ratings = ratings_df[ratings_df['user_id'] == user_id]\n",
    "        rated_books = user_ratings.sort_values(by='rating', ascending=False).head(3)['book_id'].tolist()\n",
    "\n",
    "        # Get tags for the top 3 rated books\n",
    "        relevant_tags = books_tagged[books_tagged['book_id'].isin(rated_books)]['tag_name']\n",
    "        all_words = \" \".join(relevant_tags.tolist()).split()\n",
    "        word_counts = Counter(all_words)\n",
    "        top_tags = [w for w, c in word_counts.most_common(5)]\n",
    "        if not top_tags:\n",
    "            continue\n",
    "\n",
    "        user_test_genre = \", \".join(top_tags)\n",
    "\n",
    "        # Get the user's wishlist (goal of evaluation)\n",
    "        wishlist = to_read_test_df[to_read_test_df['user_id'] == user_id]['book_id'].tolist()\n",
    "        if not wishlist:\n",
    "            continue\n",
    "\n",
    "        # Get recommendations\n",
    "        recommendations = recommend_by_multiple_genres(user_test_genre, top_n=top_n)\n",
    "        recommended_books = recommendations['book_id'].tolist()\n",
    "        \n",
    "        # Print table for recommended books and wishlist\n",
    "        # print(f\"User ID: {user_id}\")\n",
    "        # print(\"Recommended Books:\")\n",
    "        # print(pd.DataFrame(recommendations[['book_id', 'title', 'authors']]))\n",
    "        # print(\"\\nWishlist:\")\n",
    "        # print(pd.DataFrame(to_read_test[to_read_test['user_id'] == user_id][['book_id']]))\n",
    "        # print(\"-\" * 50)\n",
    "        \n",
    "        # Calculate DCG score\n",
    "        dcg_score = 0\n",
    "        for i, book_id in enumerate(recommended_books):\n",
    "            if book_id in wishlist:  # check if the recommended book is in the wishlist\n",
    "                print(f\"Recommended Book ID {book_id} is in the wishlist!\")\n",
    "                dcg_score += 1 / math.log2(i + 2)  # DCG formula\n",
    "\n",
    "        total_dcg_score += dcg_score\n",
    "        user_count += 1\n",
    "\n",
    "    # Calculate average DCG score\n",
    "    average_dcg_score = total_dcg_score / user_count if user_count > 0 else 0\n",
    "    return average_dcg_score\n",
    "\n",
    "evaluation_score = evaluate_dcg(eligible_users, ratings_df, books_tagged, to_read_test_df)\n",
    "print(f\"Average DCG Score: {evaluation_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating NDCG:  37%|███▋      | 6002/16150 [00:41<01:10, 144.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m     average_ndcg_score \u001b[38;5;241m=\u001b[39m total_ndcg_score \u001b[38;5;241m/\u001b[39m user_count \u001b[38;5;28;01mif\u001b[39;00m user_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m average_ndcg_score\n\u001b[0;32m---> 69\u001b[0m evaluation_score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_ndcg\u001b[49m\u001b[43m(\u001b[49m\u001b[43meligible_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratings_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbooks_tagged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_read_test_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage NDCG Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluation_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 46\u001b[0m, in \u001b[0;36mevaluate_ndcg\u001b[0;34m(eligible_users, ratings_df, books_tagged, to_read_test, top_n)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Get recommendations\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mrecommend_by_multiple_genres\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_test_genre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m recommended_books \u001b[38;5;241m=\u001b[39m recommendations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Calculate DCG score\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m, in \u001b[0;36mrecommend_by_multiple_genres\u001b[0;34m(user_genres, top_n)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease input at least one genre!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Embed each genre separately\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m genre_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenre_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Compute average embedding (user profile)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m user_embedding \u001b[38;5;241m=\u001b[39m genre_embeddings\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:623\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 623\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    625\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:690\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    689\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:393\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    391\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 393\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:977\u001b[0m, in \u001b[0;36mXLMRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    975\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 977\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    990\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:632\u001b[0m, in \u001b[0;36mXLMRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    621\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    622\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    623\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    629\u001b[0m         output_attentions,\n\u001b[1;32m    630\u001b[0m     )\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:563\u001b[0m, in \u001b[0;36mXLMRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    560\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    561\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 563\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:575\u001b[0m, in \u001b[0;36mXLMRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 575\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:473\u001b[0m, in \u001b[0;36mXLMRobertaIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 473\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/numpy_lab/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def evaluate_ndcg(eligible_users, ratings_df, books_tagged, to_read_test, top_n=100):\n",
    "    \"\"\"\n",
    "    Evaluate the NDCG score for recommendations.\n",
    "\n",
    "    Parameters:\n",
    "    - eligible_users: DataFrame containing eligible user IDs.\n",
    "    - ratings_df: DataFrame containing user ratings.\n",
    "    - books_tagged: DataFrame containing books with their tags.\n",
    "    - to_read_test: DataFrame containing users' wishlist books.\n",
    "    - top_n: Number of recommendations to consider for NDCG calculation.\n",
    "\n",
    "    Returns:\n",
    "    - average_ndcg_score: The average NDCG score across all users.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables for evaluation\n",
    "    total_ndcg_score = 0\n",
    "    user_count = 0\n",
    "\n",
    "    # Loop through all eligible users with tqdm progress bar\n",
    "    for user_id in tqdm(eligible_users['user_id'], desc=\"Evaluating NDCG\"):\n",
    "        # Get books rated by the user\n",
    "        user_ratings = ratings_df[ratings_df['user_id'] == user_id]\n",
    "        rated_books = user_ratings.sort_values(by='rating', ascending=False).head(3)['book_id'].tolist()\n",
    "\n",
    "        # Get tags for the top 3 rated books\n",
    "        relevant_tags = books_tagged[books_tagged['book_id'].isin(rated_books)]['tag_name']\n",
    "        all_words = \" \".join(relevant_tags.tolist()).split()\n",
    "        word_counts = Counter(all_words)\n",
    "        top_tags = [w for w, c in word_counts.most_common(5)]\n",
    "        if not top_tags:\n",
    "            continue\n",
    "\n",
    "        user_test_genre = \", \".join(top_tags)\n",
    "\n",
    "        # Get the user's wishlist (goal of evaluation)\n",
    "        wishlist = to_read_test_df[to_read_test_df['user_id'] == user_id]['book_id'].tolist()\n",
    "        if not wishlist:\n",
    "            continue\n",
    "\n",
    "        # Get recommendations\n",
    "        recommendations = recommend_by_multiple_genres(user_test_genre, top_n=top_n)\n",
    "        recommended_books = recommendations['book_id'].tolist()\n",
    "\n",
    "        # Calculate DCG score\n",
    "        dcg_score = 0\n",
    "        for i, book_id in enumerate(recommended_books):\n",
    "            if book_id in wishlist:  # check if the recommended book is in the wishlist\n",
    "                dcg_score += 1 / math.log2(i + 2)  # DCG formula\n",
    "\n",
    "        # Calculate IDCG score\n",
    "        idcg_score = 0\n",
    "        for i in range(min(len(wishlist), top_n)):\n",
    "            idcg_score += 1 / math.log2(i + 2)  # Ideal DCG formula\n",
    "\n",
    "        # Calculate NDCG score\n",
    "        ndcg_score = dcg_score / idcg_score if idcg_score > 0 else 0\n",
    "        total_ndcg_score += ndcg_score\n",
    "        user_count += 1\n",
    "\n",
    "    # Calculate average NDCG score\n",
    "    average_ndcg_score = total_ndcg_score / user_count if user_count > 0 else 0\n",
    "    return average_ndcg_score\n",
    "\n",
    "evaluation_score = evaluate_ndcg(eligible_users, ratings_df, books_tagged, to_read_test_df)\n",
    "print(f\"Average NDCG Score: {evaluation_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating NDCG: 100%|██████████| 3996/3996 [03:25<00:00, 19.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global Average NDCG@10 = 0.4389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "def compute_ndcg_at_k(ground_truth_ids, recommended_ids, user_data, k=10):\n",
    "    \"\"\"\n",
    "    Compute NDCG for a single user at cutoff k, manually.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    ground_truth_ids : set\n",
    "        Set of book_ids the user rated >= 5 (i.e. 'relevant').\n",
    "    recommended_ids : list\n",
    "        List of recommended book_ids in the final top-k (rank order).\n",
    "    user_data : pd.DataFrame\n",
    "        The subset of the ratings DataFrame for this user alone\n",
    "        (so we can look up the actual rating for each book).\n",
    "    k : int\n",
    "        The number of items to consider (already ensured recommended_ids has up to k items).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ndcg_val : float\n",
    "        The NDCG for this user at k.\n",
    "    df_debug : pd.DataFrame\n",
    "        A table showing rank, book_id, actual rating, item-level DCG\n",
    "        (so we can see how DCG is accumulated).\n",
    "    \"\"\"\n",
    "    recommended_top_k = recommended_ids[:k]\n",
    "    \n",
    "    # 1) Build a binary relevance list: 1 if rating >=5, else 0\n",
    "    relevance = [1 if b in ground_truth_ids else 0 for b in recommended_top_k]\n",
    "    \n",
    "    # 2) Compute DCG for each rank i in [0..k-1], storing item-level contributions\n",
    "    dcg_values = []\n",
    "    for i, rel in enumerate(relevance):\n",
    "        rank = i + 1  # rank is 1-based\n",
    "        dcg_i = (2 ** rel - 1) / math.log2(rank + 1)\n",
    "        dcg_values.append(dcg_i)\n",
    "    \n",
    "    dcg = sum(dcg_values)\n",
    "    \n",
    "    # 3) Compute IDCG by sorting relevance in descending order\n",
    "    ideal_relevance = sorted(relevance, reverse=True)\n",
    "    idcg_values = []\n",
    "    for i, rel in enumerate(ideal_relevance):\n",
    "        rank = i + 1\n",
    "        idcg_i = (2 ** rel - 1) / math.log2(rank + 1)\n",
    "        idcg_values.append(idcg_i)\n",
    "    \n",
    "    idcg = sum(idcg_values)\n",
    "    ndcg_val = dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    # Build a debug DataFrame\n",
    "    debug_rows = []\n",
    "    for i, book_id in enumerate(recommended_top_k):\n",
    "        rank = i + 1\n",
    "        # Actual rating from user_data\n",
    "        row = user_data[user_data['book_id'] == book_id]\n",
    "        rating = row['rating'].values[0] if not row.empty else 0\n",
    "        \n",
    "        debug_rows.append({\n",
    "            'Rank': rank,\n",
    "            'book_id': book_id,\n",
    "            'User Rating': rating,\n",
    "            'DCG Contribution': dcg_values[i]\n",
    "        })\n",
    "        \n",
    "    df_debug = pd.DataFrame(debug_rows)\n",
    "    \n",
    "    return ndcg_val, df_debug\n",
    "\n",
    "def evaluate_ndcg(ratings_subset, top_n=10):\n",
    "    \"\"\"\n",
    "    For each user:\n",
    "      1) Gather user's relevant books (rating >=5).\n",
    "      2) Build a 'genre query' from top-5 tags of those relevant books.\n",
    "      3) Get a larger set of recommended items using `recommend_by_multiple_genres`.\n",
    "      4) Skip any book that the user has not rated (rating=0).\n",
    "      5) Keep collecting items (in rank order) up to `top_n`.\n",
    "      6) Compute NDCG (manually) and store it.\n",
    "      7) Print a table showing rank, book_id, user rating, DCG contribution for each user.\n",
    "    Finally, return the average NDCG@k across users.\n",
    "    \"\"\"\n",
    "    user_ids = ratings_subset['user_id'].unique()\n",
    "    ndcg_list = []\n",
    "\n",
    "    # Pre-build a dictionary from book_id -> book_id\n",
    "    if 'book_id' not in books_tagged.columns:\n",
    "        raise KeyError(\"'book_id' column is missing in books_tagged DataFrame.\")\n",
    "    \n",
    "    # Map from book_id to book_id\n",
    "    # (If your data is different, adjust accordingly.)\n",
    "    gr2id = {}\n",
    "    for gid in books_tagged['book_id']:\n",
    "        gr2id[gid] = gid  # If they are truly the same, or else do a real map\n",
    "\n",
    "    # Alternatively, if books_tagged *does* have a separate 'book_id' column:\n",
    "    #   gr2id = dict(zip(books_tagged['book_id'], books_tagged['book_id']))\n",
    "\n",
    "    all_debug_tables = []  # to store or display if you want\n",
    "\n",
    "    for uid in tqdm(user_ids, desc=\"Evaluating NDCG\"):\n",
    "        # -- A) Get user data\n",
    "        user_data = ratings_subset[ratings_subset['user_id'] == uid].copy()\n",
    "        \n",
    "        # relevant_books = rated >= 5\n",
    "        relevant_books = set(user_data[user_data['rating'] >= 5]['book_id'].unique())\n",
    "        if len(relevant_books) == 0:\n",
    "            continue  # no relevant => skip\n",
    "\n",
    "        # -- B) Build a naive 'genre query' from top 5 tags of relevant books\n",
    "        #    You may need to use a suitable merge or direct indexing. \n",
    "        #    Below: if the 'book_id' in books_tagged is the same as\n",
    "        #    the user's 'book_id', it’s simpler, but typically you might need\n",
    "        #    a separate map or join if columns differ.  \n",
    "        #    We'll assume the columns line up or you can adapt as needed.\n",
    "\n",
    "        relevant_tags = books_tagged[books_tagged['book_id'].isin(relevant_books)]['tag_name']\n",
    "        all_words = \" \".join(relevant_tags.tolist()).split()\n",
    "        word_counts = Counter(all_words)\n",
    "        top_5_tags = [w for w, c in word_counts.most_common(5)]\n",
    "        if not top_5_tags:\n",
    "            continue\n",
    "        \n",
    "        user_genres = \", \".join(top_5_tags)\n",
    "\n",
    "        # -- C) Get a larger set of recommendations\n",
    "        #    We'll ask for top_n * 5 to have enough items to skip from.\n",
    "        recs = recommend_by_multiple_genres(user_genres, top_n=top_n * 5)\n",
    "        # recs should have 'book_id' in rank order\n",
    "        recommended_gids = recs['goodreads_book_id'].tolist()\n",
    "\n",
    "        # -- D) Build a final list that ONLY includes items user rated >0\n",
    "        #       i.e. skip rating=0. Keep collecting until we have `top_n`.\n",
    "        final_recs = []\n",
    "        for gid in recommended_gids:\n",
    "            bk_id = gr2id.get(gid, None)\n",
    "            if bk_id is None:\n",
    "                continue  # not in dictionary\n",
    "\n",
    "            # Check user's rating\n",
    "            row = user_data[user_data['book_id'] == bk_id]\n",
    "            user_rating = row['rating'].values[0] if not row.empty else 0\n",
    "            if user_rating > 0:\n",
    "                final_recs.append(bk_id)\n",
    "                if len(final_recs) == top_n:\n",
    "                    break\n",
    "\n",
    "        if len(final_recs) < 1:\n",
    "            # No recommended items that user actually rated => skip\n",
    "            continue\n",
    "\n",
    "        # -- E) Compute NDCG for these final recommendations\n",
    "        ndcg_val, df_debug = compute_ndcg_at_k(\n",
    "            ground_truth_ids=relevant_books,\n",
    "            recommended_ids=final_recs,\n",
    "            user_data=user_data,\n",
    "            k=top_n\n",
    "        )\n",
    "        ndcg_list.append(ndcg_val)\n",
    "\n",
    "        first = True\n",
    "        if first:\n",
    "            all_debug_tables.append(df_debug)\n",
    "            first = False\n",
    "        # Print a debug table for this user\n",
    "        print(f\"\\nUser: {uid}  (NDCG@{top_n} = {ndcg_val:.4f})\")\n",
    "        print(df_debug.to_string(index=False))  # or display as you like\n",
    "        print(\"----------------------------------------------------\")\n",
    "\n",
    "    # -- F) Return the average\n",
    "    if ndcg_list:\n",
    "        return np.mean(ndcg_list)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# Example usage:\n",
    "avg_ndcg = evaluate_ndcg(filtered_ratings, top_n=10)\n",
    "print(f\"\\nGlobal Average NDCG@10 = {avg_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top users with the most rated books: [28158, 7563, 24143, 37834, 6630]\n",
      "No data found for user 6630.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id  book_id  rating\n",
      "0        19009       56       2\n",
      "1        10484     5500       3\n",
      "2         3106      456       3\n",
      "3        44407     3647       5\n",
      "4         5419     6753       3\n",
      "...        ...      ...     ...\n",
      "74115     9814     5006       4\n",
      "74116    39590       12       5\n",
      "74117    38866     3070       5\n",
      "74118    50769      345       2\n",
      "74119    22529     2405       3\n",
      "\n",
      "[74120 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(filtered_ratings_20p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum all model\n",
    "import pandas as pd\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load data\n",
    "clean_folder = \"clean\"\n",
    "books_df = pd.read_csv(os.path.join(clean_folder, \"books.csv\"))\n",
    "book_tags_df = pd.read_csv(os.path.join(clean_folder, \"book_tags.csv\"))\n",
    "tags_df = pd.read_csv(os.path.join(clean_folder, \"tags.csv\"))\n",
    "\n",
    "# # Standardize column name for merging\n",
    "# book_tags_df.rename(columns={'book_id': 'book_id'}, inplace=True)\n",
    "\n",
    "# # Rename 'book_id' in books_df to 'book_id' for merging\n",
    "# books_df.rename(columns={'book_id': 'book_id'}, inplace=True)\n",
    "\n",
    "# Merge book_tags with tag names\n",
    "book_tags_merged = pd.merge(book_tags_df, tags_df, on='tag_id', how='inner')\n",
    "\n",
    "# Merge with books\n",
    "books_with_tags = pd.merge(\n",
    "    books_df[['book_id', 'title', 'authors', 'average_rating',\n",
    "              'ratings_count', 'original_publication_year', 'language_code']],\n",
    "    book_tags_merged,\n",
    "    on='book_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Group by book and concatenate tag names into a single string\n",
    "books_tagged = books_with_tags.groupby('book_id').agg({\n",
    "    'title': 'first',\n",
    "    'authors': 'first',\n",
    "    'original_publication_year': 'first',\n",
    "    'language_code': 'first',\n",
    "    'tag_name': lambda x: ' '.join(set(x))  # deduplicated tag list\n",
    "}).reset_index()\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('BAAI/bge-m3')\n",
    "\n",
    "# Encode tag text for each book\n",
    "books_tagged['tag_text'] = books_tagged['tag_name']\n",
    "book_tag_embeddings0 = model.encode(books_tagged['tag_text'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "def recommend_by_multiple_genres(user_genres, top_n=10):\n",
    "    \"\"\"\n",
    "    user_genres: str — comma-separated genres, e.g., \"Fantasy, Mystery, Romance\"\n",
    "    top_n: int — number of results to return\n",
    "    \"\"\"\n",
    "    # Parse and clean genres\n",
    "    genre_list = [g.strip() for g in user_genres.split(',') if g.strip()]\n",
    "    \n",
    "    if not genre_list:\n",
    "        raise ValueError(\"Please input at least one genre!\")\n",
    "\n",
    "    # Embed each genre separately\n",
    "    genre_embeddings = model.encode(genre_list, convert_to_tensor=True)\n",
    "\n",
    "    # Compute average embedding (user profile)\n",
    "    user_embedding = genre_embeddings.mean(dim=0)\n",
    "\n",
    "    # Compute cosine similarity with all books\n",
    "    scores = util.pytorch_cos_sim(user_embedding, book_tag_embeddings0)[0]\n",
    "    top_results = scores.topk(top_n)\n",
    "\n",
    "    # Extract matching rows\n",
    "    results = books_tagged.iloc[top_results[1].cpu().numpy()].copy()\n",
    "    results['similarity'] = top_results[0].cpu().numpy()\n",
    "    return results[[\n",
    "        'book_id', 'title', 'authors', 'original_publication_year',\n",
    "        'language_code', 'similarity'\n",
    "    ]]\n",
    "    \n",
    "def recommend_by_multiple_genres(user_genres, top_n=10):\n",
    "    \"\"\"\n",
    "    user_genres: str — comma-separated genres, e.g., \"Fantasy, Mystery, Romance\"\n",
    "    top_n: int — number of results to return\n",
    "    \"\"\"\n",
    "    # Parse and clean genres\n",
    "    genre_list = [g.strip() for g in user_genres.split(',') if g.strip()]\n",
    "    \n",
    "    if not genre_list:\n",
    "        raise ValueError(\"Please input at least one genre!\")\n",
    "\n",
    "    # Embed each genre separately\n",
    "    genre_embeddings = model.encode(genre_list, convert_to_tensor=True)\n",
    "\n",
    "    # Compute average embedding (user profile)\n",
    "    user_embedding = genre_embeddings.mean(dim=0)\n",
    "\n",
    "    # Compute cosine similarity with all books\n",
    "    scores = util.pytorch_cos_sim(user_embedding, book_tag_embeddings0)[0]\n",
    "    top_results = scores.topk(top_n)\n",
    "\n",
    "    # Extract matching rows\n",
    "    results = books_tagged.iloc[top_results[1].cpu().numpy()].copy()\n",
    "    results['similarity'] = top_results[0].cpu().numpy()\n",
    "    return results[[\n",
    "        'book_id', 'title', 'authors', 'original_publication_year',\n",
    "        'language_code', 'similarity'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numpy_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
