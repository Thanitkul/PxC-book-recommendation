{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f635510e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   book_id                                            title  \\\n",
      "0        1          The Hunger Games (The Hunger Games, #1)   \n",
      "1      133  Anne of Green Gables (Anne of Green Gables, #1)   \n",
      "2       39   A Game of Thrones (A Song of Ice and Fire, #1)   \n",
      "3       31                                         The Help   \n",
      "4       15                        The Diary of a Young Girl   \n",
      "5      157                               Green Eggs and Ham   \n",
      "6      358                        Oh, The Places You'll Go!   \n",
      "7      110   A Clash of Kings  (A Song of Ice and Fire, #2)   \n",
      "8       47                                   The Book Thief   \n",
      "9      364                  How the Grinch Stole Christmas!   \n",
      "\n",
      "                                             authors     score  \n",
      "0                                    Suzanne Collins  3.962984  \n",
      "1                                    L.M. Montgomery  3.947359  \n",
      "2                                 George R.R. Martin  3.936706  \n",
      "3                                   Kathryn Stockett  3.926117  \n",
      "4  Anne Frank, Eleanor Roosevelt, B.M. Mooyaart-D...  3.921723  \n",
      "5                                Dr. Seuss, לאה נאור  3.884909  \n",
      "6                                          Dr. Seuss  3.876628  \n",
      "7                                 George R.R. Martin  3.874859  \n",
      "8                                       Markus Zusak  3.867569  \n",
      "9                                          Dr. Seuss  3.865638  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class UserBasedCFOptimized:\n",
    "    def __init__(self, clean_folder=\"clean\", k=40):\n",
    "        # 1) Load ratings & books\n",
    "        ratings = pd.read_csv(os.path.join(clean_folder, \"ratings.csv\"))\n",
    "        self.books = pd.read_csv(os.path.join(clean_folder, \"books.csv\"))\n",
    "\n",
    "        # 2) Build user×item CSR  CSR is faster for sparse matrix operations\n",
    "        #    than COO, which is faster for building the matrix\n",
    "        row = ratings.user_id.values - 1\n",
    "        col = ratings.book_id.values - 1\n",
    "        data = ratings.rating.values\n",
    "        self.n_users = row.max() + 1\n",
    "        self.n_items = col.max() + 1\n",
    "        self.R_csr   = sparse.csr_matrix(\n",
    "            (data, (row, col)),\n",
    "            shape=(self.n_users, self.n_items)\n",
    "        )\n",
    "\n",
    "        # 3) Compute each user’s mean μ_u\n",
    "        nz_counts     = (self.R_csr != 0).sum(1).A1    # ratings per user\n",
    "        self.mu       = (self.R_csr.sum(1).A1 / nz_counts)\n",
    "\n",
    "        # 4) Demean only nonzeros via COO\n",
    "        R_coo         = self.R_csr.tocoo(copy=True)\n",
    "        R_coo.data    = R_coo.data.astype(np.float64)\n",
    "        R_coo.data   -= self.mu[R_coo.row]\n",
    "        self.Rd      = R_coo.tocsr()\n",
    "        self.Rd.eliminate_zeros()\n",
    "\n",
    "        # 5) Build sparse kNN graph on users\n",
    "        knn_graph = NearestNeighbors(\n",
    "            n_neighbors=k+1,    # include self\n",
    "            metric=\"cosine\",\n",
    "            algorithm=\"brute\",\n",
    "            n_jobs=-1\n",
    "        ).fit(self.Rd).kneighbors_graph(\n",
    "            self.Rd,\n",
    "            mode=\"distance\"\n",
    "        )\n",
    "        knn_graph.setdiag(0)\n",
    "        knn_graph.eliminate_zeros()\n",
    "        knn_graph.data = 1.0 - knn_graph.data  # distance → similarity\n",
    "        self.S = knn_graph.tocsr()             # shape: (n_users × n_users)\n",
    "\n",
    "    def recommend(self, user_id, top_n=10):\n",
    "        u = user_id - 1\n",
    "\n",
    "        # 6) Extract this user’s demeaned ratings\n",
    "        u_vec = self.Rd[u, :].toarray().ravel()   # Δr_{u,i}\n",
    "        seen  = u_vec != 0\n",
    "\n",
    "        # cold-start: no ratings → most popular books\n",
    "        if not seen.any():\n",
    "            pop = self.R_csr.sum(0).A1\n",
    "            idx = np.argsort(pop)[::-1][:top_n]\n",
    "            return (\n",
    "                self.books.set_index(\"book_id\")\n",
    "                          .loc[idx+1, [\"title\",\"authors\"]]\n",
    "                          .assign(score=pop[idx])\n",
    "                          .reset_index()\n",
    "            )\n",
    "\n",
    "        # 7) Vectorized prediction\n",
    "        #   numerator = S[u,:] ⋅ Δr vectors  → shape (n_items,)\n",
    "        #   denominator = sum |S[u,:]|      → scalar\n",
    "        s_u   = self.S.getrow(u)                # sparse row of length n_users\n",
    "        num   = s_u.dot(self.Rd).toarray().ravel()\n",
    "        den   = np.abs(s_u.data).sum() + 1e-9\n",
    "        preds = self.mu[u] + num/den            # shape (n_items,)\n",
    "\n",
    "        # 8) Mask out already-seen & pick top_n\n",
    "        preds[seen] = -np.inf\n",
    "        top_idx  = np.argpartition(preds, -top_n)[-top_n:]\n",
    "        top_idx  = top_idx[np.argsort(preds[top_idx])[::-1]]\n",
    "\n",
    "        return (\n",
    "            self.books.set_index(\"book_id\")\n",
    "                      .loc[top_idx+1, [\"title\",\"authors\"]]\n",
    "                      .assign(score=preds[top_idx])\n",
    "                      .reset_index()\n",
    "        )\n",
    "\n",
    "# ─── USAGE ─────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    model = UserBasedCFOptimized(clean_folder=\"clean\", k=40)\n",
    "    print(model.recommend(user_id=4, top_n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9959361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DCG (user-based): 100%|██████████| 7479/7479 [00:19<00:00, 378.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average DCG@10 (user-based): 1.2930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_dcg_user_cf(model, to_read_test_path, top_n=1000):\n",
    "    \"\"\"\n",
    "    Evaluate average DCG@top_n for a user-based CF model against a wishlist.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : UserBasedCFOptimized\n",
    "        Fitted CF model with .recommend(user_id, top_n) → DataFrame including 'book_id'\n",
    "    to_read_test_path : str\n",
    "        Path to 'to_read_test.csv' containing ['user_id','book_id']\n",
    "    top_n : int\n",
    "        Number of recommendations to score per user\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean DCG@top_n over all users in the test set\n",
    "    \"\"\"\n",
    "    to_read = pd.read_csv(to_read_test_path)\n",
    "    users   = to_read['user_id'].unique()\n",
    "    \n",
    "    total_dcg = 0.0\n",
    "    count     = 0\n",
    "\n",
    "    for uid in tqdm(users, desc=\"Evaluating DCG (user-based)\"):\n",
    "        # ground-truth wishlist for this user\n",
    "        wish = to_read.loc[to_read['user_id'] == uid, 'book_id'].tolist()\n",
    "        if not wish:\n",
    "            continue\n",
    "\n",
    "        # get CF recommendations\n",
    "        recs    = model.recommend(user_id=uid, top_n=top_n)\n",
    "        rec_ids = recs['book_id'].tolist()\n",
    "\n",
    "        # compute DCG: 1/log2(rank+2) for each hit\n",
    "        dcg = 0.0\n",
    "        for rank, bid in enumerate(rec_ids):\n",
    "            if bid in wish:\n",
    "                dcg += 1.0 / math.log2(rank + 2)\n",
    "\n",
    "        total_dcg += dcg\n",
    "        count     += 1\n",
    "\n",
    "    return (total_dcg / count) if count > 0 else 0.0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) import or define your user-based model\n",
    "\n",
    "    # 2) instantiate & fit\n",
    "    model = UserBasedCFOptimized(clean_folder=\"clean\", k=40)\n",
    "\n",
    "    # 3) evaluate on the test wishlist\n",
    "    test_path = os.path.join(\"clean\", \"to_read_test.csv\")\n",
    "    avg_dcg   = evaluate_dcg_user_cf(model, test_path, top_n=10)\n",
    "\n",
    "    print(f\"Average DCG@10 (user-based): {avg_dcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58a7e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating NDCG (user-based): 100%|██████████| 1071/1071 [00:01<00:00, 589.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NDCG@10 (user-based): 0.0180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_ndcg_user_cf(model, to_read_test_path, top_n=10):\n",
    "    \"\"\"\n",
    "    Evaluate average NDCG@top_n for a user-based CF model.\n",
    "    \"\"\"\n",
    "    to_read = pd.read_csv(to_read_test_path)\n",
    "    users   = to_read['user_id'].unique()\n",
    "    \n",
    "    total_ndcg = 0.0\n",
    "    count      = 0\n",
    "\n",
    "    for uid in tqdm(users, desc=\"Evaluating NDCG (user-based)\"):\n",
    "        wish = to_read.loc[to_read['user_id'] == uid, 'book_id'].tolist()[:8]\n",
    "        if not wish:\n",
    "            continue\n",
    "\n",
    "        recs     = model.recommend(user_id=uid, top_n=top_n)\n",
    "        rec_ids  = recs['book_id'].tolist()\n",
    "\n",
    "        dcg = sum(\n",
    "            1.0 / math.log2(rank + 2)\n",
    "            for rank, bid in enumerate(rec_ids)\n",
    "            if bid in wish\n",
    "        )\n",
    "\n",
    "        idcg = sum(\n",
    "            1.0 / math.log2(i + 2)\n",
    "            for i in range(min(len(wish), top_n))\n",
    "        )\n",
    "\n",
    "        ndcg = (dcg / idcg) if idcg > 0 else 0.0\n",
    "        total_ndcg += ndcg\n",
    "        count      += 1\n",
    "\n",
    "\n",
    "    return (total_ndcg / count) if count > 0 else 0.0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model     = UserBasedCFOptimized(clean_folder=\"clean\", k=40)\n",
    "    test_path = os.path.join(\"clean\", \"to_read_test.csv\")\n",
    "    avg_ndcg  = evaluate_ndcg_user_cf(model, test_path, top_n=10)\n",
    "    print(f\"Average NDCG@10 (user-based): {avg_ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbff91f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7379.84it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7487.83it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7404.59it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6576.81it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7562.13it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7627.07it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7639.96it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7721.59it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7741.06it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7627.19it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7626.63it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7029.72it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7655.54it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7321.78it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7451.69it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7547.57it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6769.06it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6871.15it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6908.79it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7285.60it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7659.63it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7259.40it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6981.34it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7109.09it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7041.00it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7473.17it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7634.24it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7536.90it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7607.24it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7533.50it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7571.42it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7280.61it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7550.12it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7462.37it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7666.43it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7378.94it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7541.72it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7471.17it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7725.76it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7524.66it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7701.66it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7575.33it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7531.80it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7681.60it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7735.02it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7488.57it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7589.19it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7738.49it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7328.79it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7659.03it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7619.75it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7608.32it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7679.12it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6528.10it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7448.32it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7650.59it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7842.42it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7586.87it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7463.62it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6976.74it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6750.41it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7496.49it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7266.98it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6881.44it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6684.65it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7430.94it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7545.50it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6754.41it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6714.44it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7587.35it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7271.66it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7586.07it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7728.33it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7650.61it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7585.32it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7430.39it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7265.15it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7616.27it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7456.10it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7627.28it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7755.58it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7483.33it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6751.98it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7225.01it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7669.60it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7748.47it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7647.10it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7696.89it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7723.59it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7635.21it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7769.49it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7647.55it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7531.58it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7701.55it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7631.14it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 7506.68it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6944.58it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6706.49it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6587.53it/s]\n",
      "Evaluating NDCG (random): 100%|██████████| 1071/1071 [00:00<00:00, 6642.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NDCG@10 (random, 100 iterations): 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def evaluate_ndcg_random(to_read_test_path, top_n=10, n_items=10000):\n",
    "    \"\"\"\n",
    "    Evaluate average NDCG@top_n for a random recommendation model.\n",
    "    \"\"\"\n",
    "    to_read = pd.read_csv(to_read_test_path)\n",
    "    users   = to_read['user_id'].unique()\n",
    "    \n",
    "    total_ndcg = 0.0\n",
    "    count      = 0\n",
    "\n",
    "    for uid in tqdm(users, desc=\"Evaluating NDCG (random)\"):\n",
    "        wish = to_read.loc[to_read['user_id'] == uid, 'book_id'].tolist()[:8]\n",
    "        if not wish:\n",
    "            continue\n",
    "\n",
    "        # Generate random recommendations\n",
    "        rec_ids = random.sample(range(1, n_items + 1), top_n)\n",
    "\n",
    "        dcg = sum(\n",
    "            1.0 / math.log2(rank + 2)\n",
    "            for rank, bid in enumerate(rec_ids)\n",
    "            if bid in wish\n",
    "        )\n",
    "\n",
    "        idcg = sum(\n",
    "            1.0 / math.log2(i + 2)\n",
    "            for i in range(min(len(wish), top_n))\n",
    "        )\n",
    "\n",
    "        ndcg = (dcg / idcg) if idcg > 0 else 0.0\n",
    "        total_ndcg += ndcg\n",
    "        count      += 1\n",
    "\n",
    "    return (total_ndcg / count) if count > 0 else 0.0\n",
    "\n",
    "\n",
    "# Evaluate random model\n",
    "# avg_ndcg_random = evaluate_ndcg_random(test_path, top_n=100, n_items=model.n_items)\n",
    "# print(f\"Average NDCG@10 (random): {avg_ndcg_random:.4f}\")\n",
    "\n",
    "# Average the result over 100 iterations\n",
    "ndcg_random_results = [\n",
    "    evaluate_ndcg_random(test_path, top_n=100, n_items=model.n_items)\n",
    "    for _ in range(100)\n",
    "]\n",
    "avg_ndcg_random_100 = sum(ndcg_random_results) / len(ndcg_random_results)\n",
    "print(f\"Average NDCG@10 (random, 100 iterations): {avg_ndcg_random_100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d608c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
